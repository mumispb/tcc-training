{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6a316c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 18:40:47.811089: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-09 18:40:47.882561: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d3f82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out corrupted images\n",
    "\n",
    "import os\n",
    "\n",
    "num_skipped = 0\n",
    "for folder_name in (\"Apple___Apple_scab\",\"Apple___Black_rot\",\"Apple___Cedar_apple_rust\",\"Apple___healthy\",\n",
    "                    \"Background_without_leaves\",\n",
    "                    \"Blueberry___healthy\",\n",
    "                    \"Cherry___healthy\",\"Cherry___Powdery_mildew\",\n",
    "                    \"Corn___Cercospora_leaf_spot_Gray_leaf_spot\",\"Corn___Common_rust\",\"Corn___healthy\",\"Corn___Northern_Leaf_Blight\",\n",
    "                    \"Grape___Black_rot\",\"Grape___Esca_Black_Measles\",\"Grape___healthy\",\"Grape___Leaf_blight_Isariopsis_Leaf_Spot\",\n",
    "                    \"Orange___Haunglongbing_Citrus_greening\",\n",
    "                    \"Peach___Bacterial_spot\",\"Peach___healthy\",\n",
    "                    \"Pepper___bell_Bacterial_spot\", \"Pepper___bell_healthy\", \n",
    "                    \"Potato___Early_blight\",\"Potato___healthy\", \"Potato___Late_blight\",\n",
    "                    \"Raspberry___healthy\",\n",
    "                    \"Soybean___healthy\", \n",
    "                    \"Squash___Powdery_mildew\",\n",
    "                    \"Strawberry___healthy\",\"Strawberry___Leaf_scorch\",\n",
    "                    \"Tomato___Target_Spot\", \"Tomato___Tomato_mosaic_virus\",\n",
    "                   \"Tomato___Tomato_Yellow_Leaf_Curl_Virus\", \"Tomato___Bacterial_spot\", \"Tomato___Early_blight\",\n",
    "                   \"Tomato___healthy\", \"Tomato___Late_blight\", \"Tomato___Leaf_Mold\", \"Tomato___Septoria_leaf_spot\",\n",
    "                   \"Tomato___Spider_mites_Two_spotted_spider_mite\" ):\n",
    "    folder_path = os.path.join(\"dataset/without_augmentation\", folder_name)\n",
    "    print(folder_path)\n",
    "    for fname in os.listdir(folder_path):\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        try:\n",
    "            fobj = open(fpath, \"rb\")\n",
    "            is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n",
    "        finally:\n",
    "            fobj.close()\n",
    "\n",
    "        if not is_jfif:\n",
    "            num_skipped += 1\n",
    "            # Delete corrupted image\n",
    "            os.remove(fpath)\n",
    "\n",
    "print(\"Deleted %d images\" % num_skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb26315",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (256, 256)\n",
    "batch_size = 64\n",
    "\n",
    "train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"dataset/without_augmentation\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ea4499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using k-fold cross-validation\n",
    "\n",
    "image_size = (256, 256)\n",
    "batch_size = 64\n",
    "\n",
    "ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"dataset/without_augmentation\",\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',\n",
    "    seed=1337\n",
    ")\n",
    "\n",
    "k = 4\n",
    "i = 1\n",
    "\n",
    "# Split the dataset into k folds using tf.data.Dataset methods\n",
    "dataset_size = len(ds)\n",
    "print('Dataset size')\n",
    "print(dataset_size)\n",
    "fold_size = dataset_size // k\n",
    "print('Fold size')\n",
    "print(fold_size)\n",
    "\n",
    "train_ds = []\n",
    "val_ds = []\n",
    "\n",
    "start = i * fold_size\n",
    "print(start)\n",
    "end = (i + 1) * fold_size\n",
    "print(end)\n",
    "\n",
    "# Create training and validation datasets for the current fold\n",
    "val_ds = ds.skip(start).take(fold_size)\n",
    "train_ds = ds.take(start).concatenate(ds.skip(end))\n",
    "\n",
    "print('Val size')\n",
    "print(val_ds.cardinality())\n",
    "print('Train size')\n",
    "print(train_ds.cardinality())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f947f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset arrays using the 3-datasets approach\n",
    "\n",
    "image_size = (256, 256)\n",
    "batch_size = 32\n",
    "\n",
    "train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"dataset/without_augmentation\",\n",
    "    validation_split=0.3,\n",
    "    subset=\"both\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "val_batches = val_ds.cardinality()\n",
    "print(val_batches)\n",
    "test_ds = val_ds.take((1*val_batches) // 3)\n",
    "val_ds = val_ds.skip((1*val_batches) // 3)\n",
    "print(train_ds.cardinality())\n",
    "print(test_ds.cardinality())\n",
    "print(val_ds.cardinality())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef67a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print quantity of images in each dataset to check\n",
    "\n",
    "train_ds_images = tf.data.experimental.cardinality(train_ds).numpy() * batch_size\n",
    "\n",
    "print(\"Quantity of images of train ds:\", train_ds_images)\n",
    "\n",
    "train_ds_images = 0\n",
    "for batch in train_ds:\n",
    "    train_ds_images += batch[0].shape[0]\n",
    "\n",
    "print(\"Quantity of images in train_ds:\", train_ds_images)\n",
    "\n",
    "val_ds_images = 0\n",
    "for batch in val_ds:\n",
    "    val_ds_images += batch[0].shape[0]\n",
    "\n",
    "print(\"Quantity of images in val_ds:\", val_ds_images)\n",
    "\n",
    "test_ds_images = 0\n",
    "for batch in test_ds:\n",
    "    test_ds_images += batch[0].shape[0]\n",
    "\n",
    "print(\"Quantity of images in test_ds:\", test_ds_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82d530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classes' names\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(\"Class names:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43000c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (x, tf.one_hot(y, 39)))\n",
    "val_ds = val_ds.map(lambda x, y: (x, tf.one_hot(y, 39)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ec629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_images = tf.data.experimental.cardinality(train_ds).numpy() * batch_size\n",
    "\n",
    "print(\"Quantity of images of train ds:\", train_ds_images)\n",
    "\n",
    "val_ds_images = tf.data.experimental.cardinality(val_ds).numpy() * batch_size\n",
    "\n",
    "print(\"Quantity of images of val ds:\", val_ds_images)\n",
    "\n",
    "train_ds_images = 0\n",
    "for batch in train_ds:\n",
    "    train_ds_images += batch[0].shape[0]\n",
    "\n",
    "print(\"Quantity of images in train_ds:\", train_ds_images)\n",
    "\n",
    "val_ds_images = 0\n",
    "for batch in val_ds:\n",
    "    val_ds_images += batch[0].shape[0]\n",
    "\n",
    "print(\"Quantity of images in val_ds:\", val_ds_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda17de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of train dataset item\n",
    "\n",
    "print(train_ds.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a848672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in val_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        #plt.title(int(labels[i]))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a34f3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation definition\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        layers.RandomBrightness(0.3),\n",
    "        layers.RandomZoom(0.4),\n",
    "        layers.RandomContrast(0.3),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21ecefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply `data_augmentation` to the training images.\n",
    "augmented_train_ds = train_ds.map(\n",
    "    lambda img, label: (data_augmentation(img), label),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "\n",
    "# Count the number of original and augmented images in the datasets.\n",
    "original_count = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "augmented_count = tf.data.experimental.cardinality(augmented_train_ds).numpy()\n",
    "\n",
    "# Calculate the number of augmented images.\n",
    "num_augmented_images = augmented_count - original_count\n",
    "\n",
    "print(\"Number of original images:\", original_count)\n",
    "print(\"Number of augmented count:\", augmented_count)\n",
    "print(\"Number of augmented images:\", num_augmented_images)\n",
    "\n",
    "# Prefetching samples in GPU memory helps maximize GPU utilization.\n",
    "\n",
    "prefetched_train_ds = augmented_train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "prefetched_val_ds = val_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3723b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU-augmented - Prefetching samples in GPU memory helps maximize GPU utilization.\n",
    "\n",
    "data_augmentation_test = keras.Sequential(\n",
    "    [\n",
    "       layers.RandomRotation(0), \n",
    "    ]\n",
    ")\n",
    "\n",
    "train_ds = train_ds.map(\n",
    "    lambda img, label: (data_augmentation_test(img), label),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "\n",
    "prefetched_train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "prefetched_val_ds = val_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1427634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # x = data_augmentation(inputs)\n",
    "\n",
    "    # Entry block\n",
    "    # x = layers.Rescaling(1.0 / 255)(x)\n",
    "    x = layers.Rescaling(1.0 / 255)(inputs)\n",
    "    x = layers.Conv2D(64, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    for size in [128, 256, 512]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    activation = \"softmax\"\n",
    "    units = num_classes\n",
    "    \n",
    "\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model = make_model(input_shape=image_size + (3,), num_classes=39)\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a376fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal training\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.History(),\n",
    "    keras.callbacks.ModelCheckpoint(\"3datasets_1e-3_manyGpuAug_stride2_batch32_64filters_categorical/{epoch}.keras\")\n",
    "]\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "history = model.fit(\n",
    "    prefetched_train_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=prefetched_val_ds,\n",
    ")\n",
    "\n",
    "with open(\"3datasets_1e-3_manyGpuAug_stride2_batch32_64filters_categorical/training_history.pkl\", 'wb') as file:\n",
    "    pickle.dump(history.history, file)\n",
    "    \n",
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig(\"3datasets_1e-3_manyGpuAug_stride2_batch32_64filters_categorical/accuracy_plot.png\")\n",
    "# summarize history for loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig(\"3datasets_1e-3_manyGpuAug_stride2_batch32_64filters_categorical/loss_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111e893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal training with k-fold\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(ds)):\n",
    "    print(f\"Fold: {fold+1}\")\n",
    "    \n",
    "    # Create training and validation datasets for this fold\n",
    "    train_ds = ds.take(train_index)\n",
    "    val_ds = ds.take(val_index)\n",
    "\n",
    "    # Reset the model and compile it\n",
    "    model = create_model()  # Replace with your model creation code\n",
    "    model.compile(...)  # Replace with your model compilation code\n",
    "\n",
    "    # Train the model on the training dataset\n",
    "    model.fit(train_ds, ...)\n",
    "\n",
    "    # Evaluate the model on the validation dataset\n",
    "    model.evaluate(val_ds, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load interrupted training\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filePath = \"1e-2_manyGpuAug_stride2_batch64_128filters-200e/100.keras\"\n",
    "\n",
    "interruptedModel = keras.models.load_model(filePath)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.History(),\n",
    "    keras.callbacks.ModelCheckpoint(\"1e-2_manyGpuAug_stride2_batch64_128filters-200e/{epoch}.keras\"),\n",
    "]\n",
    "\n",
    "history = interruptedModel.fit(\n",
    "    prefetched_train_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=prefetched_val_ds,\n",
    ")\n",
    "\n",
    "with open('1e-2_manyGpuAug_stride2_batch64_128filters-200e/training_history.pkl', 'wb') as file:\n",
    "    pickle.dump(history.history, file)\n",
    "    \n",
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig('1e-2_manyGpuAug_stride2_batch64_128filters-200e/accuracy_plot.png')\n",
    "# summarize history for loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig('1e-2_manyGpuAug_stride2_batch64_128filters-200e/loss_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a9f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replot training data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Load the training history\n",
    "with open('1e-3_manyGpuAug_stride2_batch64_128filters-ended/training_history.pkl', 'rb') as file:\n",
    "    history2 = pickle.load(file)\n",
    "\n",
    "# Retrieve accuracy and loss history\n",
    "accuracy = history2['accuracy']\n",
    "loss = history2['loss']\n",
    "val_accuracy = history2['val_accuracy']\n",
    "val_loss = history2['val_loss']\n",
    "\n",
    "# Plot accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(accuracy)\n",
    "plt.plot(val_accuracy)\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0,1)\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5ede97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"model_1.keras\")\n",
    "image_size = (256, 256)\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb0ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "import numpy as np\n",
    "\n",
    "img = keras.utils.load_img(\n",
    "    \"dataset/without_augmentation/Pepper___bell_healthy/image (5).JPG\", target_size=image_size\n",
    ")\n",
    "img_array = keras.utils.img_to_array(img)\n",
    "\n",
    "print(\"Img array\")\n",
    "print(img_array)\n",
    "\n",
    "img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "print(predictions[0])\n",
    "\n",
    "#class_index = np.argmax(predictions[0])\n",
    "#class_name = class_names[class_index] \n",
    "\n",
    "k = 5\n",
    "top_classes = np.argsort(predictions[0])[::-1][:k]  # Get indices of top k classes\n",
    "top_scores = predictions[0][top_classes]\n",
    "\n",
    "for i in range(k):\n",
    "    class_index = top_classes[i]\n",
    "    confidence = top_scores[i]\n",
    "    class_name = class_names[class_index]\n",
    "    print(f\"Class {class_name}: Confidence = {confidence:.2%}\")\n",
    "#score = float(predictions[0])\n",
    "#print(f\"The score of the image is {score}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15395bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_19e_10-2_adam.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daa84b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of test_dataset\n",
    "\n",
    "saved_model_path = \"3datasets_1e-3_manyGpuAug_stride2_batch64_64filters_moreFilters_categorical/100.keras\"\n",
    "model = tf.keras.models.load_model(saved_model_path)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(val_ds)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "912eed4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Found 55447 files belonging to 39 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 18:44:22.709572: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-09 18:44:22.731976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-09 18:44:22.732225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-09 18:44:22.734392: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-09 18:44:22.734495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-09 18:44:22.734576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-09 18:44:23.410201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-09 18:44:23.410636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-09 18:44:23.410654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-07-09 18:44:23.410838: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-09 18:44:23.410981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21256 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size\n",
      "867\n",
      "Fold size\n",
      "216\n",
      "0\n",
      "216\n",
      "Val size\n",
      "tf.Tensor(216, shape=(), dtype=int64)\n",
      "Train size\n",
      "tf.Tensor(651, shape=(), dtype=int64)\n",
      "De novo: 0\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 18:44:24.688830: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [55447]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-07-09 18:44:24.689131: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_25' with dtype int32 and shape [55447]\n",
      "\t [[{{node Placeholder/_25}}]]\n",
      "2023-07-09 18:44:31.108444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-07-09 18:44:33.265098: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-07-09 18:44:33.269665: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x347c6460 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-09 18:44:33.269699: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2023-07-09 18:44:33.281631: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-07-09 18:44:33.451922: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "651/651 [==============================] - ETA: 0s - loss: 1.6253 - accuracy: 0.5493"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 18:45:51.666788: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [55447]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-07-09 18:45:51.667264: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int32 and shape [55447]\n",
      "\t [[{{node Placeholder/_4}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "651/651 [==============================] - 97s 128ms/step - loss: 1.6253 - accuracy: 0.5493 - val_loss: 5.2826 - val_accuracy: 0.3348\n",
      "Epoch 2/100\n",
      "651/651 [==============================] - 84s 126ms/step - loss: 0.7883 - accuracy: 0.7625 - val_loss: 0.8106 - val_accuracy: 0.7772\n",
      "Epoch 3/100\n",
      "651/651 [==============================] - 84s 126ms/step - loss: 0.4870 - accuracy: 0.8485 - val_loss: 2.9958 - val_accuracy: 0.5339\n",
      "Epoch 4/100\n",
      "651/651 [==============================] - 85s 126ms/step - loss: 0.3557 - accuracy: 0.8885 - val_loss: 3.5254 - val_accuracy: 0.4855\n",
      "Epoch 5/100\n",
      "651/651 [==============================] - 85s 127ms/step - loss: 0.2723 - accuracy: 0.9131 - val_loss: 1.0023 - val_accuracy: 0.7595\n",
      "Epoch 6/100\n",
      "651/651 [==============================] - 84s 126ms/step - loss: 0.2340 - accuracy: 0.9261 - val_loss: 1.3421 - val_accuracy: 0.6929\n",
      "Epoch 7/100\n",
      "651/651 [==============================] - 85s 127ms/step - loss: 0.1973 - accuracy: 0.9387 - val_loss: 0.2881 - val_accuracy: 0.9151\n",
      "Epoch 8/100\n",
      "651/651 [==============================] - 84s 126ms/step - loss: 0.1713 - accuracy: 0.9464 - val_loss: 0.6847 - val_accuracy: 0.8379\n",
      "Epoch 9/100\n",
      "651/651 [==============================] - 85s 126ms/step - loss: 0.1563 - accuracy: 0.9495 - val_loss: 0.3309 - val_accuracy: 0.9059\n",
      "Epoch 10/100\n",
      "651/651 [==============================] - 84s 126ms/step - loss: 0.1296 - accuracy: 0.9600 - val_loss: 0.9207 - val_accuracy: 0.7922\n",
      "Epoch 11/100\n",
      "651/651 [==============================] - 84s 126ms/step - loss: 0.1381 - accuracy: 0.9563 - val_loss: 0.4443 - val_accuracy: 0.8851\n",
      "Epoch 12/100\n",
      "651/651 [==============================] - 84s 126ms/step - loss: 0.1189 - accuracy: 0.9619 - val_loss: 0.4217 - val_accuracy: 0.8946\n",
      "Epoch 13/100\n",
      "651/651 [==============================] - 84s 126ms/step - loss: 0.1055 - accuracy: 0.9665 - val_loss: 0.2774 - val_accuracy: 0.9217\n",
      "Epoch 14/100\n",
      "651/651 [==============================] - 84s 126ms/step - loss: 0.1038 - accuracy: 0.9669 - val_loss: 0.2886 - val_accuracy: 0.9154\n",
      "Epoch 15/100\n",
      "651/651 [==============================] - 82s 123ms/step - loss: 0.0896 - accuracy: 0.9716 - val_loss: 0.7364 - val_accuracy: 0.8428\n",
      "Epoch 16/100\n",
      "651/651 [==============================] - 78s 117ms/step - loss: 0.0900 - accuracy: 0.9721 - val_loss: 0.1567 - val_accuracy: 0.9508\n",
      "Epoch 17/100\n",
      "651/651 [==============================] - 78s 116ms/step - loss: 0.0842 - accuracy: 0.9719 - val_loss: 0.0818 - val_accuracy: 0.9758\n",
      "Epoch 18/100\n",
      "651/651 [==============================] - 79s 117ms/step - loss: 0.0746 - accuracy: 0.9760 - val_loss: 0.0805 - val_accuracy: 0.9769\n",
      "Epoch 19/100\n",
      "651/651 [==============================] - 78s 117ms/step - loss: 0.0718 - accuracy: 0.9772 - val_loss: 0.4333 - val_accuracy: 0.8992\n",
      "Epoch 20/100\n",
      "651/651 [==============================] - 78s 117ms/step - loss: 0.0741 - accuracy: 0.9767 - val_loss: 1.3999 - val_accuracy: 0.7697\n",
      "Epoch 21/100\n",
      "651/651 [==============================] - 79s 117ms/step - loss: 0.0651 - accuracy: 0.9794 - val_loss: 1.0638 - val_accuracy: 0.8303\n",
      "Epoch 22/100\n",
      "651/651 [==============================] - 79s 117ms/step - loss: 0.0722 - accuracy: 0.9767 - val_loss: 0.9856 - val_accuracy: 0.7907\n",
      "Epoch 23/100\n",
      "651/651 [==============================] - 79s 117ms/step - loss: 0.0543 - accuracy: 0.9822 - val_loss: 0.2260 - val_accuracy: 0.9408\n",
      "Epoch 24/100\n",
      "651/651 [==============================] - 79s 117ms/step - loss: 0.0593 - accuracy: 0.9806 - val_loss: 0.1008 - val_accuracy: 0.9723\n",
      "Epoch 25/100\n",
      "651/651 [==============================] - 79s 117ms/step - loss: 0.0532 - accuracy: 0.9831 - val_loss: 0.7432 - val_accuracy: 0.8678\n",
      "Epoch 26/100\n",
      "651/651 [==============================] - 78s 117ms/step - loss: 0.0619 - accuracy: 0.9799 - val_loss: 2.7394 - val_accuracy: 0.6787\n",
      "Epoch 27/100\n",
      "651/651 [==============================] - 78s 117ms/step - loss: 0.0522 - accuracy: 0.9832 - val_loss: 0.1480 - val_accuracy: 0.9623\n",
      "Epoch 28/100\n",
      "651/651 [==============================] - 79s 117ms/step - loss: 0.0535 - accuracy: 0.9831 - val_loss: 0.1392 - val_accuracy: 0.9582\n",
      "Epoch 29/100\n",
      "651/651 [==============================] - 78s 117ms/step - loss: 0.0499 - accuracy: 0.9840 - val_loss: 0.4486 - val_accuracy: 0.9026\n",
      "Epoch 30/100\n",
      "651/651 [==============================] - 78s 117ms/step - loss: 0.0471 - accuracy: 0.9847 - val_loss: 0.1454 - val_accuracy: 0.9621\n",
      "Epoch 31/100\n",
      "651/651 [==============================] - 79s 117ms/step - loss: 0.0489 - accuracy: 0.9846 - val_loss: 0.3286 - val_accuracy: 0.9188\n",
      "Epoch 32/100\n",
      "651/651 [==============================] - 78s 117ms/step - loss: 0.0458 - accuracy: 0.9853 - val_loss: 0.1564 - val_accuracy: 0.9560\n",
      "Epoch 33/100\n",
      "651/651 [==============================] - 78s 117ms/step - loss: 0.0416 - accuracy: 0.9867 - val_loss: 0.0971 - val_accuracy: 0.9720\n",
      "Epoch 34/100\n",
      "651/651 [==============================] - 78s 117ms/step - loss: 0.0382 - accuracy: 0.9876 - val_loss: 0.4399 - val_accuracy: 0.9122\n",
      "Epoch 35/100\n",
      "651/651 [==============================] - 78s 117ms/step - loss: 0.0415 - accuracy: 0.9872 - val_loss: 0.2298 - val_accuracy: 0.9445\n",
      "Epoch 36/100\n",
      "651/651 [==============================] - 78s 117ms/step - loss: 0.0358 - accuracy: 0.9886 - val_loss: 0.0454 - val_accuracy: 0.9879\n",
      "Epoch 37/100\n",
      "651/651 [==============================] - 78s 116ms/step - loss: 0.0417 - accuracy: 0.9865 - val_loss: 0.4044 - val_accuracy: 0.9066\n",
      "Epoch 38/100\n",
      "651/651 [==============================] - 78s 117ms/step - loss: 0.0408 - accuracy: 0.9863 - val_loss: 0.1736 - val_accuracy: 0.9587\n",
      "Epoch 39/100\n",
      "651/651 [==============================] - 79s 117ms/step - loss: 0.0413 - accuracy: 0.9871 - val_loss: 0.2507 - val_accuracy: 0.9454\n",
      "Epoch 40/100\n",
      "651/651 [==============================] - 79s 117ms/step - loss: 0.0311 - accuracy: 0.9901 - val_loss: 0.1375 - val_accuracy: 0.9596\n",
      "Epoch 41/100\n",
      "651/651 [==============================] - 78s 117ms/step - loss: 0.0341 - accuracy: 0.9896 - val_loss: 0.2001 - val_accuracy: 0.9522\n",
      "Epoch 42/100\n",
      "651/651 [==============================] - 78s 117ms/step - loss: 0.0383 - accuracy: 0.9871 - val_loss: 0.0929 - val_accuracy: 0.9731\n",
      "Epoch 43/100\n",
      "651/651 [==============================] - 78s 116ms/step - loss: 0.0385 - accuracy: 0.9879 - val_loss: 0.1654 - val_accuracy: 0.9628\n",
      "Epoch 44/100\n",
      "651/651 [==============================] - 78s 117ms/step - loss: 0.0345 - accuracy: 0.9892 - val_loss: 0.2440 - val_accuracy: 0.9459\n",
      "Epoch 45/100\n",
      "651/651 [==============================] - 78s 117ms/step - loss: 0.0290 - accuracy: 0.9904 - val_loss: 0.2448 - val_accuracy: 0.9383\n",
      "Epoch 46/100\n",
      " 50/651 [=>............................] - ETA: 1:02 - loss: 0.0652 - accuracy: 0.9812"
     ]
    }
   ],
   "source": [
    "#K-fold complete\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "k = 4\n",
    "folds = [0,1]\n",
    "\n",
    "for i in folds:\n",
    "    print(f'Fold: {i}')\n",
    "    \n",
    "    # Using k-fold cross-validation\n",
    "\n",
    "    image_size = (256, 256)\n",
    "    batch_size = 64\n",
    "\n",
    "    ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        \"dataset/without_augmentation\",\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode='categorical',\n",
    "        seed=1337\n",
    "    )\n",
    "\n",
    "    # Split the dataset into k folds using tf.data.Dataset methods\n",
    "    dataset_size = len(ds)\n",
    "    print('Dataset size')\n",
    "    print(dataset_size)\n",
    "    fold_size = dataset_size // k\n",
    "    print('Fold size')\n",
    "    print(fold_size)\n",
    "\n",
    "    start = i * fold_size\n",
    "    print(start)\n",
    "    end = (i + 1) * fold_size\n",
    "    print(end)\n",
    "\n",
    "    # Create training and validation datasets for the current fold\n",
    "    val_ds = ds.skip(start).take(fold_size)\n",
    "    train_ds = ds.take(start).concatenate(ds.skip(end))\n",
    "\n",
    "    print('Val size')\n",
    "    print(val_ds.cardinality())\n",
    "    print('Train size')\n",
    "    print(train_ds.cardinality())\n",
    "\n",
    "    # Data augmentation definition\n",
    "\n",
    "    data_augmentation = keras.Sequential(\n",
    "        [\n",
    "            layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "            layers.RandomBrightness(0.3),\n",
    "            layers.RandomZoom(0.4),\n",
    "            layers.RandomContrast(0.3),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # GPU-augmented - Prefetching samples in GPU memory helps maximize GPU utilization.\n",
    "\n",
    "    data_augmentation_test = keras.Sequential(\n",
    "        [\n",
    "           layers.RandomRotation(0), \n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_ds = train_ds.map(\n",
    "        lambda img, label: (data_augmentation_test(img), label),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    )\n",
    "\n",
    "    prefetched_train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    prefetched_val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    def make_model(input_shape, num_classes):\n",
    "        inputs = keras.Input(shape=input_shape)\n",
    "        # x = data_augmentation(inputs)\n",
    "\n",
    "        # Entry block\n",
    "        # x = layers.Rescaling(1.0 / 255)(x)\n",
    "        x = layers.Rescaling(1.0 / 255)(inputs)\n",
    "        x = layers.Conv2D(64, 3, strides=2, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "        previous_block_activation = x  # Set aside residual\n",
    "\n",
    "        for size in [128, 256, 512]:\n",
    "            x = layers.Activation(\"relu\")(x)\n",
    "            x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "\n",
    "            x = layers.Activation(\"relu\")(x)\n",
    "            x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "\n",
    "            x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "            # Project residual\n",
    "            residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
    "                previous_block_activation\n",
    "            )\n",
    "            x = layers.add([x, residual])  # Add back residual\n",
    "            previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "        x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "        activation = \"softmax\"\n",
    "        units = num_classes\n",
    "\n",
    "\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        outputs = layers.Dense(units, activation=activation)(x)\n",
    "        return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "    model = make_model(input_shape=image_size + (3,), num_classes=39)\n",
    "    keras.utils.plot_model(model, show_shapes=True)\n",
    "\n",
    "    # Normal training\n",
    "\n",
    "    epochs = 100\n",
    "    print(f\"De novo: {i}\")\n",
    "\n",
    "    callbacks = [\n",
    "        #tf.keras.callbacks.History(),\n",
    "        keras.callbacks.ModelCheckpoint(f'fold_{i}' + \"_1e-2_manyGpuAug_stride2_batch64_64filters_categorical/{epoch}.keras\")\n",
    "    ]\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-2),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    history = model.fit(\n",
    "        prefetched_train_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=prefetched_val_ds,\n",
    "    )\n",
    "\n",
    "    with open(f'fold_{i}_1e-2_manyGpuAug_stride2_batch64_64filters_categorical/training_history.pkl', 'wb') as file:\n",
    "        pickle.dump(history.history, file)\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig(f'fold_{i}_1e-2_manyGpuAug_stride2_batch64_64filters_categorical/accuracy_plot.png')\n",
    "    # summarize history for loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig(f'fold_{i}_1e-2_manyGpuAug_stride2_batch64_64filters_categorical/loss_plot.png')\n",
    "    \n",
    "    val_accuracy = model.evaluate(val_ds)\n",
    "\n",
    "    with open(f'fold_{i}_1e-2_manyGpuAug_stride2_batch64_64filters_categorical/result', 'w') as file:\n",
    "        # Write the variable value to the file\n",
    "        file.write(str(val_accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
